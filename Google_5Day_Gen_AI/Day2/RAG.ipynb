{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U -q \"google-generativeai>=0.8.3\" chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/embedding-gecko-001\n",
      "models/embedding-001\n",
      "models/text-embedding-004\n"
     ]
    }
   ],
   "source": [
    "for m in genai.list_models():\n",
    "    if \"embedding\" in m.name:\n",
    "        print(m.name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCUMENT_1 = \"Self-attention allows a model to focus on different parts of a sequence by computing relevance scores between tokens. Each token gathers information from others, helping the model understand context and relationships within the sequence. This mechanism enables efficient handling of long-range dependencies, making it crucial for tasks like language translation and text generation.\"\n",
    "DOCUMENT_2 = \"The Transformer architecture introduced by Google in 2017 revolutionized the field of natural language processing. It introduced the concept of self-attention, which allows the model to process sequences in parallel, significantly increasing processing speed. The Transformer architecture consists of multiple layers of self-attention mechanisms, each layer processing the entire sequence at once, rather than processing one token at a time as in previous models like LSTM.\"\n",
    "\n",
    "documents = [DOCUMENT_1, DOCUMENT_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Self-attention allows a model to focus on different parts of a sequence by computing relevance scores between tokens. Each token gathers information from others, helping the model understand context and relationships within the sequence. This mechanism enables efficient handling of long-range dependencies, making it crucial for tasks like language translation and text generation.', 'The Transformer architecture introduced by Google in 2017 revolutionized the field of natural language processing. It introduced the concept of self-attention, which allows the model to process sequences in parallel, significantly increasing processing speed. The Transformer architecture consists of multiple layers of self-attention mechanisms, each layer processing the entire sequence at once, rather than processing one token at a time as in previous models like LSTM.']\n"
     ]
    }
   ],
   "source": [
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
    "from google.api_core import retry\n",
    "\n",
    "\n",
    "class GeminiEmbeddingFunction(EmbeddingFunction):\n",
    "    # Specify whether to generate embeddings for documents, or queries\n",
    "    document_mode = True\n",
    "\n",
    "    def __call__(self, input: Documents) -> Embeddings:\n",
    "        if self.document_mode:\n",
    "            embedding_task = \"retrieval_document\"\n",
    "        else:\n",
    "            embedding_task = \"retrieval_query\"\n",
    "\n",
    "        retry_policy = {\"retry\": retry.Retry(predicate=retry.if_transient_error)}\n",
    "\n",
    "        response = genai.embed_content(\n",
    "            model=\"models/text-embedding-004\",\n",
    "            content=input,\n",
    "            task_type=embedding_task,\n",
    "            request_options=retry_policy,\n",
    "        )\n",
    "        return response[\"embedding\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "DB_NAME='transformer'\n",
    "embed_fn = GeminiEmbeddingFunction()\n",
    "embed_fn.document_mode = True\n",
    "\n",
    "chroma_client = chromadb.Client()\n",
    "db = chroma_client.get_or_create_collection(DB_NAME, embedding_function=embed_fn)\n",
    "\n",
    "db.add(documents = documents, ids = [str(i) for i in range(len(documents))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': ['0', '1'],\n",
       " 'embeddings': array([[-0.02625349,  0.0407034 , -0.02011823, ..., -0.04470832,\n",
       "          0.03691332, -0.00053415],\n",
       "        [-0.03317715,  0.00780859, -0.02967615, ..., -0.05387708,\n",
       "          0.07014375, -0.00397073]]),\n",
       " 'documents': ['Self-attention allows a model to focus on different parts of a sequence by computing relevance scores between tokens. Each token gathers information from others, helping the model understand context and relationships within the sequence. This mechanism enables efficient handling of long-range dependencies, making it crucial for tasks like language translation and text generation.',\n",
       "  'The Transformer architecture introduced by Google in 2017 revolutionized the field of natural language processing. It introduced the concept of self-attention, which allows the model to process sequences in parallel, significantly increasing processing speed. The Transformer architecture consists of multiple layers of self-attention mechanisms, each layer processing the entire sequence at once, rather than processing one token at a time as in previous models like LSTM.'],\n",
       " 'uris': None,\n",
       " 'data': None,\n",
       " 'metadatas': [None, None],\n",
       " 'included': [<IncludeEnum.embeddings: 'embeddings'>,\n",
       "  <IncludeEnum.documents: 'documents'>,\n",
       "  <IncludeEnum.metadatas: 'metadatas'>]}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.count()\n",
    "db.peek()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Self-attention allows a model to focus on different parts of a sequence by computing relevance scores between tokens. Each token gathers information from others, helping the model understand context and relationships within the sequence. This mechanism enables efficient handling of long-range dependencies, making it crucial for tasks like language translation and text generation."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "embed_fn.document_mode = False\n",
    "query = \"What is self-attention?\"\n",
    "\n",
    "result = db.query(query_texts = [query], n_results = 1)\n",
    "[[passage]] = result[\"documents\"]\n",
    "Markdown(passage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "You are a lazy bot that answers questions using text from the reference passage included below and answer in one sentence. \n",
       "Be sure to respond in a complete sentence including all relevant background information. \n",
       "However, you are talking to technical audience, so be sure to skip easy concepts. If the passage is irrelevant to the answer, you may ignore it.\n",
       "\n",
       "QUESTION: What is self-attention?\n",
       "PASSAGE: Self-attention allows a model to focus on different parts of a sequence by computing relevance scores between tokens. Each token gathers information from others, helping the model understand context and relationships within the sequence. This mechanism enables efficient handling of long-range dependencies, making it crucial for tasks like language translation and text generation.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passage_oneline = passage.replace(\"\\n\", \" \")\n",
    "query_oneline = query.replace(\"\\n\", \" \")\n",
    "\n",
    "# This prompt is where you can specify any guidance on tone, or what topics the model should stick to, or avoid.\n",
    "prompt = f\"\"\"You are a lazy bot that answers questions using text from the reference passage included below and answer in one sentence. \n",
    "Be sure to respond in a complete sentence including all relevant background information. \n",
    "However, you are talking to technical audience, so be sure to skip easy concepts. If the passage is irrelevant to the answer, you may ignore it.\n",
    "\n",
    "QUESTION: {query_oneline}\n",
    "PASSAGE: {passage_oneline}\n",
    "\"\"\"\n",
    "Markdown(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Self-attention is a mechanism that allows a model to focus on different parts of a sequence by computing relevance scores between tokens. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = genai.GenerativeModel(\"gemini-1.5-flash-latest\")\n",
    "answer = model.generate_content(prompt)\n",
    "Markdown(answer.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dororo99",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
